\documentclass{article}

\usepackage{preamble}
\usepackage[backend=biber,style=authoryear,defernumbers=true]{biblatex}
\addbibresource{refs.bib}
\providecommand{\insertbiblabel}{}

\title{Summer of Math Exposition 4: Sampling}
\author{Ian Chen}

\begin{document}

\maketitle

Sampling is a fundamental part in statistical studies.
Instead of collecting data from a large population, which is often infeasible at worse and expensive at best, statisticians collect a \emph{representative sample} and infer conclusions about the population from the sample.
Here, we will explore algorithms to efficiently sample from data streams.

We expect a prerequisite knowledge of Big-O and elementary probability.
See \S\ref{sec:prereq} for a refresher.
% TODO: maybe add a background section?

In \S\ref{sec:problem}, we develop the problem and a simple solution.
Then, releasing the assumption that we know the population size, we explore a simple type of resevoir sampling algorithm in \S\ref{sec:bottomk} and an application in \S\ref{sec:reach}.
Finally, we refine it to optimality in \S\ref{sec:resevoir}.
Proofs for some theorems are in \S\ref{sec:proofs}, but I encourage the reader to attempt them as exercises.

\section{Problem Definition}
\label{sec:problem}

Let $D = (x_1, x_2, \ldots, x_N)$.
Let $k$ be the number of samples we wish the collect, and $N$ the size of the stream (if known).
We wish to compute a sample (w/o replacement) $S(D) \subset D$, where each $x_i$ has an equal chance of being in the sample, and $\abs{S(D)} = \min(n, \abs{D})$.
We are allowed to use the functions \random(), generating a uniform real in $[0, 1)$, and the \randint(a, b), generating a uniform integer in $[a, b]$.

% probability mass function approach
Here's an example:
% idea 1: divide the [0, 1] into intervals proportional to the weight
%   - pick a point at random
%   - then, we can draw a picture by "extending" it into the 2D pmf
%   - same as saying its the F^{-1}(U) "direct sampling" method, i.e. picking the right interval in the limit

There are a few properties of this approach that can be undesireable, which all stem from the fact that we need to store the entire data stream.
This is because we make multiple passes of the data, first in finding the sum of all the weights, and a new pass when selecting an element (and removing).
From here on, we explore resevoir sampling~(\cite{vitter85-03}), which are single-pass sampling techniques.

\section{Bottom-k Sampling}
\label{sec:bottomk}

The key idea here is that we can select a random element by assigning a random key $y_i$ for each $x_i$, and picking the minimum.
Moreover, we can select $k$ items by picking the $k$ smallest keys.

\begin{algorithm}
    \caption{Bottom-k Sampling}
    \begin{algorithmic}[1]
        \Function{GenerateSample}{$D$, $k$}
            \State Initialize a resevoir with the first $k$ keys
            \For{each $i > k$ in increasing order}
                \State $y_i \gets \random()$
                \If{$y_i$ is in the lowest $k$ keys explored so far}
                    \State Update the resevoir to include $x_i$ (and remove the largest)
                \EndIf
            \EndFor
        \EndFunction
    \end{algorithmic}
    \label{alg:bottomk}
\end{algorithm}

The correctness follows from the following theorem.
\begin{theorem}
    \label{thm:rank}
    Let $Y_1, \ldots, Y_n \simiid Y$, and suppose WLOG that they are distinct.
    Then, $\Pr{r_i \le k} = \frac{k}{n}$, where $r_i = \abs{\set{Y_j \mid Y_j \le Y_i}}$ is the rank of $Y_i$.
\end{theorem}

\subsection{Application}
\label{sec:reach}

This section requires terminology of graph theory and can be skipped.
It is adapted from \S3 of ~\cite{cohen97-12}.

For each vertex $v \in V$, we want to sample $k$ vertices (w/o replacement) from its $2$-hop neighborhood.
\cite{cohen97-12} use this sample to estimate the size of the neighborhood, but we focus more on the sampling process here.

Following the ideas developed in \S\ref{sec:bottomk}, we first assign a random key for each vertex.
Then, for each vertex, we need to compute the $k$ least keys in its neighborhood.

% TODO: algorithm
\begin{algorithm}
    \caption{Sampling from 2-Hop Neighborhood}
    \begin{algorithmic}[1]
        \Function{Sample2Hop}{$V$, $E$, $k$}
            \For{$i$ from $1 \to k$}
                \State $y_j \gets \random$
                \For{$j$ in increasing $y_j$}
                    \If{$v_j$ is done}
                        \State \textbf{continue}
                    \EndIf
                    \For{$u \in$ two-hop neighborhood of $v_j$}
                        \State Add $v_j$ to $S_u$ if $u$ has not been added to yet
                        \State Remove edges incident to $u$
                        \State Mark $u$ as done
                    \EndFor
                \EndFor
            \EndFor
            \Return $\set{S_i}$
        \EndFunction
    \end{algorithmic}
    \label{alg:sample2hop}
\end{algorithm}

\begin{theorem}
    For each vertex, we can collect samples of $k$ vertices (w/o) replacement from its 2-hop neighborhood in $O(k(n \log n + m))$ time.
\end{theorem}

\begin{proof}
    First, we find that Algorithm \ref{alg:sample2hop} correctly solves the problem because for each vertex, it finds the minimum $k$ keys in its reachability set.
    Thus, by our analysis in \S\ref{sec:bottomk}, we correctly generate a sample.

    Now, we prove the runtime.
    Consider an iteration of the algorithm.
    First, we generate random weights, which is done in $O(n)$ time, and sort them in $O(n \log n)$ time.
    Finally, we proceed by doing a reachability search, which visits every edge at most once and so takes $O(m)$ time.
    Repeating this algorithm $k$ times, we obtain a final runtime of $O(k(n \log n + m))$.
\end{proof}

\section{Improved Resevoir Sampling}
\label{sec:resevoir}

There is one more trick that we can apply to speed up the algorithm even further.
Consider how much information we need to simulate the output of Algorithm \ref{alg:bottomk}; all we need to know is which indices do we update our resevoir of $k$ smallest keys.
In fact, the gap between indices follow a geometric distribution, and Algorithm \ref{alg:bottomk} is just a complicated way of generating a geometric distribution.
Using this observation, we can avoid examining each item one by one and instead skip between items~(\cite{li94-12, efraimidis06-03}):
\begin{algorithm}
    \caption{GenerateSampleFast}
    \begin{algorithmic}[1]
        \Function{GenerateSampleFast}{$D$, $k$}
            \State Initialize a resevoir with the first $k$ keys
            \State $i \gets k + 1$
            \Comment{the current index}
            \State $M \gets \random()^{1/k}$
            \Comment{distributed like $\max_{1 \le j \le j}(\UnifD(0, 1))$}
            \State $m \gets - \ln( \random() ) / M$
            \Comment{$\GeomD(M)$, the steps before find a uniform with value $\le$ M}
            \While{$i + m \le N$}
                \State $i \gets i + m$
                \State Replace a random item in resevoir with $x_{i}$
                \State $M \gets M \cdot \random()^{1/k}$
                \Comment{distributed like $\max_{1 \le j \le j}(\UnifD(0, M))$}
                \State $m \gets - \ln( \random() ) / M$
                \Comment{$\GeomD(M)$, the steps before find a uniform with value $\le$ M}
            \EndWhile
        \EndFunction
    \end{algorithmic}
    \label{alg:samplejumps}
\end{algorithm}

\section{Prerequisites}
\label{sec:prereq}

\section{Proofs}
\label{sec:proofs}

\begin{proof}[Theorem \ref{thm:rank}]
    Suppose this.
    Then, we win.
\end{proof}

% References
\printbibliography

\end{document}
