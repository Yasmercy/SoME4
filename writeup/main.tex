\documentclass{article}

\usepackage{preamble}
\usepackage[backend=biber,style=authoryear,defernumbers=true]{biblatex}
\addbibresource{refs.bib}
\providecommand{\insertbiblabel}{}

\title{Summer of Math Exposition 4: Sampling}
\author{Ian Chen}

\begin{document}

\maketitle

Sampling is a fundamental part in statistical studies.
Instead of collecting data from a large population, which is often infeasible at worse and expensive at best, statisticians collect a \emph{representative sample} and infer conclusions about the population from the sample.
Here, we will explore algorithms to efficiently sample from data streams.

We expect a prerequisite knowledge of Big-O and elementary probability.
See \S\ref{sec:prereq} for a refresher.
% TODO: maybe add a background section?

In \S\ref{sec:problem}, we develop the problem and a simple solution.
Then, releasing the assumption that we know the population size, we explore a simple type of resevoir sampling algorithm in \S\ref{sec:bottomk} and an application in \S\ref{sec:reach}.
Finally, we refine it to optimality in \S\ref{sec:resevoir}.
Proofs for some theorems are in \S\ref{sec:proofs}, but I encourage the reader to attempt them as exercises.

\section{Problem Definition}
\label{sec:problem}

Let $D = (x_1, x_2, \ldots)$ be a data stream with weights $(w_1, w_2, \ldots)$.
Let $k$ be the number of samples we wish the collect, and $N$ the size of the stream (if known).
We wish to compute a sample $S(D) \subset D$, where $\Pr{x_i \in S(D)} \propto w_i$ and $\abs{S(D)} = \min(n, \abs{D})$.
That is, we want to sample without replacement.
We are allowed to use the \random(), generating a uniform real in $[0, 1)$, and the \randint(a, b), generating a uniform integer in $[a, b]$, functions.

% probability mass function approach
Here's an example:
% idea 1: divide the [0, 1] into intervals proportional to the weight
%   - pick a point at random
%   - then, we can draw a picture by "extending" it into the 2D pmf
%   - same as saying its the F^{-1}(U) "direct sampling" method, i.e. picking the right interval in the limit

There are a few properties of this approach that can be undesireable, which all stem from the fact that we need to store the entire data stream.
This is because we make multiple passes of the data, first in finding the sum of all the weights, and a new pass when selecting an element (and removing).
From here on, we explore resevoir sampling~(\cite{vitter85-03}), which are single-pass sampling techniques.

\section{Bottom-k Sampling}
\label{sec:bottomk}

First, let us assume unit weights, namely $w_i = 1$.
Then, we can select a random element by assigning a random key $y_i$ for each $x_i$, and picking the minimum.
Moreover, we can select $k$ items by picking the $k$ smallest keys.
The correctness is established by the following theorem.

\begin{theorem}
	\label{thm:rank}
	Let $Y_1, \ldots, Y_n \simiid Y$, and suppose WLOG that they are distinct.
	Then, $\Pr{r_i \le k} = \frac{k}{n}$, where $r_i = \abs{\set{Y_j \mid Y_j \le Y_i}}$ is the rank of $Y_i$.
\end{theorem}

Now, suppose that the weights are not unit weight, we need to generate the keys $y_i$ according to the weight $w_i$.
One approach given by~\cite{cohen07-08} is to do this using the exponential distribution.

\begin{lemma}
	Let $U_i \sim \UnifD(0, 1)$, and $E_{\theta_i} \sim \ExpD(\theta_i)$ all be independent.
	Then,
	\begin{enumerate}
		\item $-\theta_i \log U_i \sim \ExpD(\theta_i)$.
		\item $\min(E_{\theta_1}, \ldots, E_{\theta_{n}}) \sim \ExpD(\sum_i \theta_i)$
		\item $\Pr{E_i < E_j} = \theta_i / (\theta_i + \theta_j)$
		\item $\Pr{E_i > t + t_0 \mid E_i > t_0} = \Pr{E_i > t}$
	\end{enumerate}
\end{lemma}
% TODO: visual proof for this?

We generate a key according to the exponential distribution and take the items with the lowest-k keys as our sample.
\begin{algorithm}
	\caption{Bottom-K Sampling}
	\begin{algorithmic}[1]
		\Function{WeightedSample}{$D$, $k$}
			\For{each $i$ in increasing order}
				\State Generate $y_i \sim \ExpD(w_i)$
				\If{$y_i$ is in the lowest $k$ keys explored so far}
					\State Add $x_i$ to the sample
				\EndIf
			\EndFor
		\EndFunction
	\end{algorithmic}
\end{algorithm}

To show that this correctly generates a weighted random sample,
\begin{proof}

\end{proof}

\subsection{Application}
\label{sec:reach}

This section requires terminology of graph theory and can be skipped.
It is adapted from \S3 of ~\cite{cohen97-12}.

Suppose we have a simple directed graph $G = (V, E)$ with \emph{vertex weights}.
For each vertex $v \in V$, we want to sample $k$ vertices (w/o replacement) from its $2$-hop neighborhood, with probability proportional to the vertex weights.
\cite{cohen97-12} use this sample to estimate the size of the neighborhood, but we focus more on the sampling process here.

Following the ideas developed in \S\ref{sec:bottomk}, we first assign a random key for each vertex.
Then, for each vertex, we need to compute the $k$ least keys in its neighborhood.

% TODO: algorithm
\begin{algorithm}
	\caption{Sampling from 2-Hop Neighborhood}
	\begin{algorithmic}[1]
		\Function{Sample2Hop}{$V$, $E$, $k$}
			\For{$i$ from $1 \to k$}
				\State Generate $y_j \sim \ExpD(w_i)$
				\For{$j$ in increasing $y_j$}
					\If{$v_j$ is done}
						\State \textbf{continue}
					\EndIf
					\For{$u \in$ two-hop neighborhood of $v_j$}
						\State Add $v_j$ to $S_u$ if $u$ has not been added to yet
						\State Remove edges incident to $u$
						\State Mark $u$ as done
					\EndFor
				\EndFor
			\EndFor
			\Return $\set{S_i}$
		\EndFunction
	\end{algorithmic}
	\label{alg:sample2hop}
\end{algorithm}

\begin{theorem}
	For each vertex, we can collect samples of $k$ vertices (w/o) replacement from its 2-hop neighborhood in $O(k(n \log n + m))$ time.
\end{theorem}

\begin{proof}
	First, we find that Algorithm \ref{alg:sample2hop} correctly solves the problem because for each vertex, it finds the minimum $k$ keys in its reachability set.
	Thus, by our analysis in \S\ref{sec:bottomk}, we correctly generate a weighted sample.

	Now, we prove the runtime.
	Consider an iteration of the algorithm.
	First, we generate random weights, which is done in $O(n)$ time, and sort them in $O(n \log n)$ time.
	Finally, we proceed by doing a reachability search, which visits every edge at most once and so takes $O(m)$ time.
	Repeating this algorithm $k$ times, we obtain a final runtime of $O(k(n \log n + m))$.
\end{proof}

\section{Optimal Resevoir Sampling}
\label{sec:resevoir}

For optimality, see ~\cite{vitter85-03}.
See~\cite{li94-12} for unweighted resevoir sampling, and ~\cite{efraimidis06-03} for weighted sampling.

\section{Prerequisites}
\label{sec:prereq}

\section{Proofs}
\label{sec:proofs}

\begin{proof}[Theorem \ref{thm:rank}]
	Suppose this.
	Then, we win.
\end{proof}

% References
\printbibliography

\end{document}
